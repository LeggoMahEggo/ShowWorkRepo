from flask import request, jsonify, Flask
from flask_cors import CORS # MUST-HAVE
from flask_script import Manager
import json
import logging
from mas_boto import S3Manager, path_leaf
import mas_database as mas_d
from mas_exceptions import *
from mas_form import *
from mas_logging import LogOp
from mas_searches import PerformWhooshSearch, LevenshteinSearchByScore, GetFinalSearchResults, EnglishSoundexFilter, HebrewSoundexFilter, GetHebrewSoundexAnalyzer, GetEnglishSoundexAnalyzer
import os
from os import listdir
from os.path import isfile, join
import sys
import traceback

# Returns error messages generated by the server
# e: an Exception object
# logObj: the logging object
# s3Bucket(optional): the s3 bucket to upload the log files to (default None)
# NOTE: 's3Bucket' is default None in the event that connecting to s3 raises an exception
def ErrorHandler(e, logObj, s3Bucket = None):
    errMsg = type(sys.exc_info()[1]).__name__ + ": " + str(e)
    trace = traceback.format_exc()
    print(errMsg + "\n")
    print(trace)
    
    # Save logs (regular and stack trace) to file
    logFiles = []
    savePath = GetTMPDir(os.getcwd())
    logObj.LogError(errMsg)
    logObj.LogStackTrace(errMsg)
    logFiles.append(logObj.SaveLogFile("action", savePath))
    logFiles.append(logObj.SaveLogFile("exception", savePath))
    
    # If the s3 bucket is open, upload the log files to it
    if (s3Bucket is not None):
        for logFile in logFiles:
            fileData = open(join(savePath, logFile), "rb")
            s3Bucket.UploadObjectToBucket(logFile, fileData)
    
    # Return an error JSON
    errMsgs = ["Server has run into the following error: %s" % (errMsg)]
    return jsonify({"has_error": True, "error_messages": errMsgs, "stack_trace": trace})

# Returns the path to the 'tmp' directory (creating it if it doesn't exist on Windows)
# mainPath: the path where the main program files reside
# NOTE: this is due to AWS Lambda being a FaaS (Function as a Service), '/tmp/' is the only storage available
def GetTMPDir(mainPath):
    tmpPath = ""

    # Windows only (for testing)
    if (os.name == "nt"):
        tmpPath = join(mainPath, "tmp") # For downloading temporary files to
        if (not os.path.exists(tmpPath)):
            os.mkdir(tmpPath)
    else:
        tmpPath = "/tmp/" # For Linux, which is the platform AWS uses
    
    return tmpPath

# Create a S3Manager object (for handling AWS s3 stuff)
# pathToZappaSettings: the path to the zappa settings (which contains the name of the s3 bucket)
def LoadS3Manager(pathToZappaSettings):
    bucketName = ""
    if (not os.path.exists(join(pathToZappaSettings, "zappa_settings.json"))):
        raise FileNotFoundError("zappa_settings.json does not exist; cannot load s3 bucket")
        
    with open("zappa_settings.json", "r") as jsonReader:
        jsonData = json.load(jsonReader)
        bucketName = jsonData["dev"]["s3_bucket"]
        
    return S3Manager(bucketName)
    
# Downloads the Footprints database files (.csv) and index files (for Whoosh) into a folder
# s3Bucket: the s3 bucket to download from
# downloadPath: the path to download to (os-nonspecific)
def DownloadSearchFiles(s3Bucket, downloadPath):
    searchFiles = []
    downloadFiles = [obj.key for obj in s3Bucket.GetObjectsInBucket(".csv")]
    downloadFiles += [obj.key for obj in s3Bucket.GetObjectsInBucket("MAIN_")]
    for file in downloadFiles:
        s3Bucket.DownloadObjectFromBucket(file, downloadPath)
        searchFiles.append(file)
        
    return searchFiles

# Takes Footprints database files (.csv) and uploads them to the s3 bucket
# s3Bucket: the s3 bucket to upload to
# updateFiles: the files to upload
# pathOfFiles: where the files to upload exist
def UpdateDatabase(s3Bucket, updateFiles, pathOfFiles):
    
    for file in updateFiles:
        if (".csv" not in file):
            continue
    
        data = open(join(pathOfFiles, file), "rb")
        s3Bucket.UploadObjectToBucket(file, data)

# Recreates the Whoosh index used for making searches
# s3Bucket: an S3Manager object
# dbFiles: the .csv files that contain the data for Footprints
# pathOfFiles: the path to the Footprints files
def RecreateIndex(s3Bucket, dbFiles, pathOfFiles):
    
    # Load new database files and create a new index
    footprintsDF = mas_d.LoadDatabaseFromCSV(pathOfFiles, dbFiles)
    mas_d.CreateOrLoadSchemaAndGetIndex(pathOfFiles, footprintsDF, recreateIndex = True)

    # Put new index files into a list
    tmpFiles = [f for f in listdir(pathOfFiles) if isfile(join(pathOfFiles, f))]
    indexFiles = [file for file in tmpFiles if ("MAIN_" in file)]
            
    # Remove old index files, and put in the new ones (no versioning for index files)
    s3Bucket.DisableVersioning()
    oldFiles = [obj.key for obj in s3Bucket.GetObjectsInBucket("MAIN_")]
    
    # Delete old files
    for file in oldFiles:
        s3Bucket.DeleteObjectFromS3(file)
        
    # Upload new files
    for file in indexFiles:
        data = open(join(pathOfFiles, file), "rb")
        s3Bucket.UploadObjectToBucket(file, data)
    
    # Re-enable versioning
    s3Bucket.EnableVersioning()



# START
appName = "Mispelled Author Search" # Set app name

# Set app details
app = Flask(appName)
app.config["DEBUG"] = False
CORS(app, supports_credentials = True, resources = {"/api/v1/*": {"origins": "*"}}, methods = "POST") # Sets up CORS on the server

# Setup manager to control what happens when the server starts
manager = Manager(app)

# Routes
@app.route('/', methods=['GET'])
def home():
    return "There's nothing here."

@app.route('/api/v1/delete', methods=["POST"])
def delete():
    try:
        form = None
        fpMg = None # <---- this is needed in case opening a connection to s3/an error occurs before that - raises an exception  
        logObj = LogOp(request.remote_addr, request.url_rule) # Start logging object
        logObj.LogMessage("Start of execution")
        logObj.LogMessage("-" * len("Start of execution"))
        
        if ("application/json" in request.content_type):
            form = request.json
        elif ("text/plain" in request.content_type):
            form = json.loads(request.data.decode("utf-8"))
        else:
            raise TypeError("Content type must be 'application/json' or 'text/plain' (content type sent is %s)" % (request.content_type))
        
        # Get access token - without it, you CANNOT delete anything
        if ("access_token" not in form):
            raise KeyError("An access token must be provided to delete a database file! (missing 'access_token')")
        
        accessToken = str(form["access_token"])
        if (accessToken != "5qjA8lY0sRK2zl3gqTyGHfkJLPvPVJDL"):
            raise ValueError("Access token is invalid! (No deletion will be performed without a correct access token)")
           
        # Make sure that the name of the file you want to remove exists, and is of the correct filetype
        if ("remover_file" not in form):
            raise KeyError("A database file to delete must be provided! (missing 'remover_file')")
        removerFile = str(form["remover_file"])
        removerFile += "" if (".csv" in removerFile) else ".csv"
        
        # Get working directories
        path = os.getcwd()        
        tmpPath = GetTMPDir(path)

        # Open s3 manager instance
        logObj.LogMessage("Loading s3 manager object")
        fpMg = LoadS3Manager(path)
        
        # Make sure that the database file exists (should only be 1 copy)
        logObj.LogMessage("Verifying existence of '%s' in the database" % (removerFile))
        fileExists = len([obj.key for obj in fpMg.GetObjectsInBucket(removerFile, exactSearch = True)]) == 1
        if (not fileExists):
            raise FileNotFoundError("'%s' does not exist in the database" % (removerFile))
        logObj.LogMessage("Deleting '%s' from the database" % (removerFile))
        deletedFile = fpMg.DeleteObjectFromS3(removerFile)
        
        # Recreate the index
        logObj.LogMessage("Recreating Whoosh index")
        RecreateIndex(fpMg, [obj.key for obj in fpMg.GetObjectsInBucket(".csv")], tmpPath)
        logObj.LogMessage("End of execution")
        
        # Save the log file and upload it to the s3 bucket
        emptyStream = logObj.CheckIfEmpty("action")
        if (not emptyStream):
            logFile = logObj.SaveLogFile("action", tmpPath)
            fileData = open(join(tmpPath, logFile), "rb")
            fpMg.UploadObjectToBucket(logFile, fileData)
        
        return jsonify({})
        
    except Exception as e:
        return ErrorHandler(e, logObj, fpMg), 500
        
@app.route('/api/v1/update', methods=["POST"])
def update():
    try:
        form = None
        updateFile = None
        fpMg = None # <---- this is needed in case opening a connection to s3/an error occurs before that - raises an exception
        
        logObj = LogOp(request.remote_addr, request.url_rule) # Start logging object
        logObj.LogMessage("Start of execution")
        logObj.LogMessage("-" * len("Start of execution"))
        
        if ("multipart/form-data" in request.content_type):
            form = json.loads(request.form["json"])
            updateFile = request.files["file"]
            
            if (".csv" not in updateFile.filename):
                raise FileTypeError("Wrong file type! (must be '.csv', was '%s')" % (file.filename[-4:]))
        else:
            raise TypeError("Content type must be 'multipart/form-data' (content type sent is %s)" % (request.content_type))
        
        # Get access token - without it, you CANNOT update anything
        if ("access_token" not in form):
            raise ValueError("An access token must be provided to update/add a database file!")
        
        accessToken = str(form["access_token"])
        if (accessToken != "5qjA8lY0sRK2zl3gqTyGHfkJLPvPVJDL"):
            raise ValueError("Access token is invalid! (No update/addition will be performed without a correct access token)")
        
        # Get working directories
        path = os.getcwd()
        tmpPath = GetTMPDir(path)

        # Open s3 manager instance
        logObj.LogMessage("Loading s3 manager object")
        fpMg = LoadS3Manager(path)
        
        # Download (only) database files
        logObj.LogMessage("Downloading Footprints database files (.csv) to %s" % (tmpPath))
        dbFiles = [obj.key for obj in fpMg.GetObjectsInBucket(".csv")]
        for file in dbFiles:
            fpMg.DownloadObjectFromBucket(file, tmpPath)
        logObj.LogMessage("Files downloaded: %s" % (", ".join(dbFiles)))
        
        # Download the sent file
        logObj.LogMessage("Downloading sent file ('%s') to %s" % (updateFile.filename, tmpPath))
        updateFile.save(join(tmpPath, updateFile.filename))
        
        # Update the database files in the s3 bucket
        logObj.LogMessage("Uploading Footprints database file(s) to the s3 bucket")
        #finalFiles = dbFiles if (updateFile.filename in dbFiles) else dbFiles + [updateFile.filename]
        finalFiles = [updateFile.filename]
        UpdateDatabase(fpMg, finalFiles, tmpPath)
        logObj.LogMessage("Files uploaded: %s" % (", ".join(finalFiles)))
        
        # Recreate the index
        recreateFiles = dbFiles if (updateFile.filename in dbFiles) else dbFiles + [updateFile.filename]
        logObj.LogMessage("Recreating Whoosh index")
        RecreateIndex(fpMg, recreateFiles, tmpPath)
        logObj.LogMessage("End of execution")
        
        # Save the log file and upload it to the s3 bucket
        emptyStream = logObj.CheckIfEmpty("action")
        if (not emptyStream):
            logFile = logObj.SaveLogFile("action", tmpPath)
            fileData = open(join(tmpPath, logFile), "rb")
            fpMg.UploadObjectToBucket(logFile, fileData)
        
        return jsonify({})
        
    except Exception as e:
        return ErrorHandler(e, logObj, fpMg), 500


@app.route("/api/v1/search", methods = ["POST"])
def search():
    try:
        form = None
        fpMg = None # <---- this is needed in case opening a connection to s3/an error occurs before that - raises an exception
        
        # Removes the AWS LambdaHandler which prevents us from logging stuff
        root = logging.getLogger()
        if (root.handlers):
            for handler in root.handlers:
                root.removeHandler(handler)
        
        logObj = LogOp(request.remote_addr, request.url_rule) # Start logging object
        logObj.LogMessage("Start of execution")
        logObj.LogMessage("-" * len("Start of execution"))
        
        if ("application/json" in request.content_type):
            form = request.json
        elif ("text/plain" in request.content_type):
            form = json.loads(request.data.decode("utf-8"))
        else:
            raise TypeError("Content type must be 'application/json' or 'text/plain' (content type sent is %s)" % (request.content_type))
        
        # Check the form data, return errors if inputted information is incorrect
        errMsgs = CheckFormData(form)
        if (len(errMsgs) > 0):
            return jsonify({"has_error": True, "error_messages": errMsgs, "stack_trace": None}), 500
        
        # Set POST data to local variables
        textToSearch = form["search_text"]
        lang = form["lang"]
        searchTypes = form["search_types"]
        maxScore = 2.0 if ("max_score" not in form.keys()) else float(form["max_score"])
        
        # Get working directories
        path = os.getcwd()
        tmpPath = GetTMPDir(path)

        # Open s3 manager instance
        logObj.LogMessage("Loading s3 manager object")
        fpMg = LoadS3Manager(path)
        
        # Download database files and index files
        logObj.LogMessage("Downloading Footprints database files and Whoosh index files to %s" % (tmpPath))
        searchFiles = DownloadSearchFiles(fpMg, tmpPath)
        logObj.LogMessage("Files downloaded: %s" % (", ".join(searchFiles)))
        
        # Load Footprints database
        logObj.LogMessage("Loading Footprints database file into dataframe")
        footprintsDF = mas_d.LoadDatabaseFromCSV(tmpPath, [file for file in searchFiles if (".csv" in file)])
    
        # Dictionary for storing the different types of searches
        searchDicts = {"exact": None, "levenshtein": None, "soundex": None}
        
        # Perform searches
        for searchType in searchTypes:
            if (type(searchType) is not str):
                raise ValueError("Search types must be strings")
            else:
                logObj.LogMessage("Performing %s search" % (searchType))
            
            # Levenshtein search
            if (searchType == "levenshtein"):
                levenshteinDF = LevenshteinSearchByScore(textToSearch, footprintsDF, maxScore)
                searchDicts["levenshtein"] = {"searchType": "levenshtein", "df": levenshteinDF}
                continue
             
            # Other searches (exact, soundex)
            if (searchType in searchDicts.keys()):
                searchDicts[searchType] = PerformWhooshSearch(tmpPath, footprintsDF, lang, textToSearch, searchType)
            
        # Get final results 
        # List of dictionaries, containing each search performed
        logObj.LogMessage("Collating final results")
        searchesList = [searchType for searchName,searchType in searchDicts.items() if (searchType is not None)]
        resultsDict = GetFinalSearchResults(searchesList)
        logObj.LogMessage("End of execution")
        
        # Save the log file and upload it to the s3 bucket
        emptyStream = logObj.CheckIfEmpty("action")
        if (not emptyStream):
            logFile = logObj.SaveLogFile("action", tmpPath)
            fileData = open(join(tmpPath, logFile), "rb")
            fpMg.UploadObjectToBucket(logFile, fileData)
        
        # Return results as a JSON-like object
        return jsonify(resultsDict)
        
    except Exception as e:
        return ErrorHandler(e, logObj, fpMg), 500

# Run server app
if (__name__ == "__main__"):
    manager.run()